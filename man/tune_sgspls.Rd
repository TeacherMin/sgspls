% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tuning.R
\name{tune_sgspls}
\alias{tune_sgspls}
\title{Compute cross-validated mean squared prediction error for sgspls regression}
\usage{
tune_sgspls(pls_obj, sparsities = NULL, group_seq = NULL, block = "X",
  folds = 10, progressBar = TRUE, setseed = 1, scale_resp = TRUE)
}
\arguments{
\item{pls_obj}{List of parameters or object of class \code{cv.sgspls} used to perform cross validation (see examples below).}

\item{sparsities}{Matrix of sparsities, with columns corresponding to group, subgroup and individual 
sparsity levels to tune over. If it is NULL then a preselected set of sparsity levels is used.}

\item{group_seq}{a vector containing the number of groups to tune over.}

\item{block}{A string either "X" or "Y" to indicate which block to tune parameters over.}

\item{folds}{The number of folds to use in cross validation.}

\item{progressBar}{Logical, indicating if a progress bar is shown.}

\item{setseed}{False, or integer for replicating tuning parameters.}

\item{scale_resp}{Logical, the MSEP is standardised across responses (see perf function for details).}
}
\value{
\code{tune_sgspls} returns a list with class \code{cv.sgspls} with measures:
\item{result_tuning}{A matrix containing the tuning parameters and MSEP values.}
\item{best}{A vector containing the optimal tuning parameters. }
\item{parameters}{A list of the parameters for a sgspls object.}
\item{tuning_sparsities}{A matrix of group, subgroup and individual sparsities tuned over.}
\item{folds}{Number of folds used in cross validation.}
\item{min_cv}{Minimum MSEP score.}
\item{group_seq}{Groups tuned over in cross validation.}
}
\description{
Tuning function for finding the number of groups, and sparsities to select for an sgspls object.
Offers a sequential way to find optimal sparsities, and number of groups for either block.
}
\examples{

 set.seed(1)
 n = 50; p = 510; 
 size.groups = 30; size.subgroups = 5
 groupX <- ceiling(1:p / size.groups)
 subgroupX <- ceiling(1:p / size.subgroups)
 
 X = matrix(rnorm(n * p), ncol = p, nrow = n)
 
 beta <- rep(0,p)
 bSG <- -2:2; b0 <- rep(0,length(bSG))
 betaG <- c(bSG, b0, bSG, b0, bSG, b0)
 beta[1:size.groups] <- betaG
 
 y = X \%*\% beta + rnorm(n)
 
 #--------------------------------------#
 #-- Set up a basic model to tune --#
 
 cv_pls <- list(X=X, Y=y, groupX=groupX, subgroupX=subgroupX)
 
 #---------------------------------------------#
 #-- Tune over 1 to 2 groups and multiple    --#
 #-- sparsity levels for the first component --#
 
 cv_pls_comp1 <- tune_sgspls(pls_obj = cv_pls, group_seq = 1:2, scale_resp = FALSE)
 
 #-- MSEP is on the original scale for the response --#
 cv_pls_comp1$results_tuning
 cv_pls_comp1$best
 
 \dontrun{
 # Use the optimal fit for the first component and tune the second component
 cv_pls_comp2 <- tune_sgspls(pls_obj = cv_pls_comp1, group_seq = 1:2, scale_resp = FALSE)
 cv_pls_comp2$results_tuning
 cv_pls_comp2$best
 
 # Use the optimal fit for the second component and tune the third component
 cv_pls_comp3 <- tune_sgspls(pls_obj =  cv_pls_comp2, group_seq = 1:2, scale_resp = FALSE)
 cv_pls_comp3$best
 
 model <- do.call(sgspls, args = cv_pls_comp3$parameters)
 
 model
 
 model_coef <- coef(model, type = "coefficients")
 
 cbind(beta, model_coef$B[,,2])
 }
}
\references{
Liquet Benoit, Lafaye de Micheaux, Boris Hejblum, Rodolphe
  Thiebaut. A group and Sparse Group Partial Least Square approach applied in
  Genomics context. \emph{Submitted}.
}
\seealso{
\code{\link[sgspls]{sgspls}} Tuning functions \code{\link[sgspls]{calc_pve}}, \code{\link[sgspls]{tune_groups}}. 
Model performance and estimation  \code{\link[sgspls]{predict.sgspls}}, \code{\link[sgspls]{perf.sgspls}}, \code{\link[sgspls]{coef.sgspls}}
}
