% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tuning.R
\name{tune_sgspls}
\alias{tune_sgspls}
\title{Compute cross-validated mean squared prediction error for sgspls regression}
\usage{
tune_sgspls(parameters, sparsities = NULL, group_seq = NULL, block = "X",
  folds = 10, progressBar = TRUE, setseed = 1, scale_resp = TRUE)
}
\arguments{
\item{parameters}{List of parameters to use in the current PLS object (see examples below).}

\item{sparsities}{Matrix of sparsities, with columns corresponding to group, subgroup and individual 
sparsity levels to tune over. If it is NULL then a preselected set of sparsity levels is used.}

\item{group_seq}{a vector containing the number of groups to tune over.}

\item{block}{A string either "X" or "Y" to indicate which block to tune parameters over.}

\item{folds}{The number of folds to use in cross validation.}

\item{progressBar}{Logical, indicating if a progress bar is shown.}

\item{setseed}{False, or integer for replicating tuning parameters.}

\item{scale_resp}{Logical, the MSEP is standardised across responses (see perf function for details).}
}
\value{
\code{tune_sgspls} returns a list with class \code{cv.sgspls} with measures:
\item{result_tuning}{A matrix containing the tuning parameters and MSEP values.}
\item{best}{A vector containing the optimal tuning parameters. }
\item{parameters}{A list of the parameters for a sgspls object.}
\item{tuning_sparsities}{A matrix of group, subgroup and individual sparsities tuned over.}
\item{folds}{Number of folds used in cross validation.}
\item{min_cv}{Minimum MSEP score.}
\item{group_seq}{Groups tuned over in cross validation.}
}
\description{
Tuning function for finding the number of groups, and sparsities to select for an sgspls object.
Offers a sequential way to find optimal sparsities, and number of groups for either block.
}
\examples{

 set.seed(1)
 n = 50; p = 500; 
 size.groups = 30; size.subgroups = 5
 groupX <- ceiling(1:p / size.groups)
 subgroupX <- ceiling(1:p / size.subgroups)
 
 X = matrix(rnorm(n * p), ncol = p, nrow = n)
 
 beta <- rep(0,p)
 bSG <- -2:2; b0 <- rep(0,length(bSG))
 betaG <- c(bSG, b0, bSG, b0, bSG, b0)
 beta[1:size.groups] <- betaG
 
 y = X \%*\% beta + 0.1*rnorm(n)
 
 #--------------------------------------#
 #-- Set up a basic model to tune --#
 
 parameters <- list(X=X, Y=y, groupX=groupX, subgroupX=subgroupX)
 
 #---------------------------------------------#
 #-- Tune over 1 to 2 groups and multiple    --#
 #-- sparsity levels for the first component --#
 
 cv_pls_comp1 <- tune_sgspls(parameters = parameters, group_seq = 1:2, scale_resp = F)
 
 #-- MSEP is on the original scale for the response --#
 cv_pls_comp1$results_tuning
 cv_pls_comp1$best
 
 # Use the optimal fit for the first component and tune the second component
 cv_pls_comp2 <- tune_sgspls(parameters = cv_pls_comp1$parameters, group_seq = 1:2, scale_resp = F)
 cv_pls_comp2
 
 # Use the optimal fit for the second component and tune the third component
 cv_pls_comp3 <- tune_sgspls(parameters = cv_pls_comp2$parameters, group_seq = 1:2, scale_resp = F)
 
 #--------------------------------------#
 #------ Inspect tuning function  ------#
 
 # Look at the plot of validation curves
 plot(cv_pls_comp3)
 
 # get the optimal values
 cv_pls_comp3
 
 # Fit the pls model with these parameters
 model <- do.call(sgspls, args = cv_pls_comp3$parameters)
 
 # See model details
 model
 
 # get the regression coefficients
 model_coef <- coef(model, type = "coefficients", comps = 3)
 
 cbind(beta, model_coef$B)
 
 
}
\references{
Liquet Benoit, Lafaye de Micheaux, Boris Hejblum, Rodolphe
  Thiebaut. A group and Sparse Group Partial Least Square approach applied in
  Genomics context. \emph{Submitted}.
}
\seealso{
\code{\link{sgspls}} Tuning functions \code{\link{calc_pve}}, \code{\link{tune_groups}}. 
Model performance and estimation  \code{\link{predict}}, \code{\link{perf.sgspls}}, \code{\link{coeff.sgspls}}
}
